# üöÄ Guia de Testes e Demonstra√ß√£o do Banco de Dados Distribu√≠do

Este documento unifica os roteiros de teste para dois cen√°rios de apresenta√ß√£o:

1.  **M√°quina √önica:** Usando Docker e scripts de automa√ß√£o para simular um ambiente distribu√≠do localmente. Ideal para uma demonstra√ß√£o r√°pida e controlada.
2.  **M√∫ltiplas M√°quinas:** Configurando o ambiente manualmente em diferentes m√°quinas (f√≠sicas ou virtuais) para provar o funcionamento em uma rede real.

---

## Cen√°rio 1: Demonstra√ß√£o em M√°quina √önica (com Docker)

Este cen√°rio √© o mais r√°pido para apresentar, pois utiliza os scripts de automa√ß√£o que preparam todo o ambiente.

### 1.1 Preparando o Ambiente

Tudo o que voc√™ precisa para iniciar o ambiente est√° contido em um √∫nico script.

**A√ß√£o:** Abra um terminal PowerShell e execute:
```powershell
.\iniciar_ambiente.ps1
```

**O que acontece neste momento:**
1.  As depend√™ncias Python s√£o instaladas.
2.  Os tr√™s cont√™ineres Docker (um para cada n√≥ do banco de dados) s√£o iniciados.
3.  O script aguarda os bancos de dados ficarem prontos.
4.  O arquivo `config.json` √© gerado com os IPs dos cont√™ineres.
5.  O esquema de tabelas √© inicializado em cada banco de dados.
6.  Os tr√™s processos do middleware (`node.py`) s√£o iniciados em segundo plano.

**Como observar os n√≥s:**
Para ver o que cada n√≥ est√° fazendo em tempo real, abra **tr√™s terminais** e execute em cada um:

```powershell
# Terminal 1
Get-Content .\logs\node0.log -Wait

# Terminal 2
Get-Content .\logs\node1.log -Wait

# Terminal 3
Get-Content .\logs\node2.log -Wait
```

### 1.2 Demonstra√ß√£o Passo a Passo

Com o ambiente rodando e os logs vis√≠veis, siga os passos abaixo.

#### Passo 1: Verificando a Elei√ß√£o do Coordenador
**A√ß√£o:** Observe os logs.
**Resultado:** O N√≥ 2 ser√° eleito o coordenador (mensagem: `New Coordinator: 2` nos outros, e `I am the coordinator` nele).
> üé§ **Ponto da Apresenta√ß√£o:** "O algoritmo do Valent√£o elege o n√≥ de maior ID como coordenador inicial."

#### Passo 2: Realizando uma Opera√ß√£o de Escrita (INSERT)
**A√ß√£o:** Use `python client.py`, conecte-se a qualquer n√≥ (ex: `0`) e execute:
```sql
INSERT INTO users (name, email) VALUES ('Ada Lovelace', 'ada@babbage.com');
```
**Resultado:** O log do N√≥ 0 mostrar√° `Transmitting content...`, e os outros `Executing replicated query...`.
> üé§ **Ponto da Apresenta√ß√£o:** "Opera√ß√µes de escrita s√£o replicadas para todos os n√≥s para garantir a consist√™ncia."

#### Passo 3: Verificando a Replica√ß√£o
**A√ß√£o:** Use o `client.py` para executar `SELECT * FROM users;` em cada um dos tr√™s n√≥s.
**Resultado:** Todos os n√≥s retornar√£o o registro de 'Ada Lovelace'.
> üé§ **Ponto da Apresenta√ß√£o:** "A consulta retorna o mesmo resultado em todos os n√≥s, provando que o cluster est√° consistente."

#### Passo 4: Demonstrando a Toler√¢ncia a Falhas
**A√ß√£o:** Derrube o coordenador (N√≥ 2).
1.  Encontre o PID do N√≥ 2 (√© o terceiro no arquivo `node_pids.tmp`).
    ```powershell
    Get-Content .\node_pids.tmp 
    ```
2.  Encerre o processo.
    ```powershell
    Stop-Process -Id <PID_DO_NO_2>
    ```
**Resultado:** Os logs dos n√≥s 0 e 1 detectar√£o a falha (`Node 2 is down`) e eleger√£o o N√≥ 1 como novo coordenador.
> üé§ **Ponto da Apresenta√ß√£o:** "Simulamos a falha do coordenador. O sistema detectou e iniciou uma nova elei√ß√£o, mantendo-se operacional."

#### Passo 5: Verificando a Funcionalidade P√≥s-Falha
**A√ß√£o:** Com o N√≥ 2 inativo, use o `client.py` para inserir um novo registro em um n√≥ ativo (0 ou 1).
```sql
INSERT INTO users (name, email) VALUES ('Charles Babbage', 'charles@babbage.com');
```
**Resultado:** A escrita funcionar√°, e um `SELECT` nos n√≥s 0 e 1 mostrar√° o novo registro.
> üé§ **Ponto da Apresenta√ß√£o:** "Mesmo com um n√≥ a menos, o cluster continua dispon√≠vel e consistente, demonstrando alta disponibilidade."

### 1.3 Encerrando o Ambiente
**A√ß√£o:** Para limpar tudo, execute:
```powershell
.\parar_ambiente.ps1
```
---

## Cen√°rio 2: Demonstra√ß√£o em M√∫ltiplas M√°quinas

Este cen√°rio prova que o sistema funciona em um ambiente de rede real, sem Docker.

### 2.1 Pr√©-requisitos e Configura√ß√£o Manual

**Em cada uma das 3 m√°quinas:**
- [ ] Clone o reposit√≥rio do projeto.
- [ ] Instale Python 3.8+.
- [ ] Instale e configure um servidor MySQL 8.0.
- [ ] No MySQL, crie o usu√°rio `root` com senha `root` e d√™ as permiss√µes necess√°rias.
- [ ] Crie o banco de dados: `CREATE DATABASE IF NOT EXISTS bd-dist;`.
- [ ] Libere as portas `5000` (para o middleware) e `3306` (para o MySQL) no firewall.
- [ ] Instale as depend√™ncias: `pip install -r requirements.txt`.

**Configura√ß√£o Central:**
1.  Escolha uma m√°quina para ser a "principal" (onde voc√™ rodar√° o cliente).
2.  Crie o arquivo `ips.txt`, listando os IPs de rede de cada uma das 3 m√°quinas.
3.  Crie o arquivo `config.json` manualmente, com a seguinte estrutura:
    ```json
    {
      "nodes": [
        {"id": 0, "ip": "192.168.1.10", "port": 5000, "db_port": 3306},
        {"id": 1, "ip": "192.168.1.11", "port": 5000, "db_port": 3306},
        {"id": 2, "ip": "192.168.1.12", "port": 5000, "db_port": 3306}
      ]
    }
    ```
    *Substitua pelos IPs reais de suas m√°quinas.*

4.  Execute o script de inicializa√ß√£o do banco de dados em **uma** das m√°quinas (ele se conectar√° remotamente √†s outras).
    ```bash
    python init_db.py
    ```

### 2.2 Roteiro de Testes Manuais

**A√ß√£o:** Em cada m√°quina, abra um terminal e inicie o n√≥ correspondente ao seu ID.

```bash
# Na m√°quina com final de IP .10
python node.py 0

# Na m√°quina com final de IP .11
python node.py 1

# Na m√°quina com final de IP .12
python node.py 2
```

Agora, voc√™ pode seguir exatamente os mesmos passos da **Se√ß√£o 1.2 (Demonstra√ß√£o Passo a Passo)**. A l√≥gica √© id√™ntica:
- Observe a elei√ß√£o do N√≥ 2.
- Use `python client.py` de qualquer uma das m√°quinas para inserir dados.
- Observe a replica√ß√£o nos terminais de cada m√°quina.
- Para simular a falha, simplesmente use `Ctrl+C` no terminal da m√°quina do N√≥ 2.
- Observe a nova elei√ß√£o e a continuidade do sistema.

---

## üì∏ Checklist de Screenshots para Apresenta√ß√£o

| # | Captura | Demonstra |
|---|---------|-----------|
| 1 | 3 terminais (locais ou remotos) com n√≥s rodando | Arquitetura distribu√≠da |
| 2 | Log "New Coordinator: 2" | Bully Algorithm |
| 3 | Log "Node X is down" | Detec√ß√£o de falha |
| 4 | Log "Transmitting content" + "Checksum" | Replica√ß√£o + Integridade |
| 5 | Sa√≠da do `client.py` com resultado JSON | Interface do cliente |
| 6 | `SELECT` em 3 n√≥s/m√°quinas com dados id√™nticos | Consist√™ncia de dados |
| 7 | `config.json` com IPs reais (para Cen√°rio 2) | Configurabilidade |

---

## ‚ö†Ô∏è Problemas Conhecidos e Limita√ß√µes

### 1. Sincroniza√ß√£o de N√≥ Reiniciado
Quando um n√≥ √© desligado e religado, ele n√£o recebe os dados que foram inseridos durante sua aus√™ncia.
**Solu√ß√£o sugerida**: Implementar um mecanismo de *state transfer* onde um n√≥ que retorna pede ao coordenador um snapshot dos dados atuais.

### 2. Conflitos de Escrita Simult√¢nea
Se dois clientes inserem dados no mesmo momento em n√≥s diferentes, pode haver um conflito de chave prim√°ria (auto-increment).
**Solu√ß√£o sugerida**: Usar UUIDs para chaves prim√°rias ou ter o coordenador como ponto central para serializar todas as opera√ß√µes de escrita.

---

## üèÅ Conclus√£o

O middleware desenvolvido demonstra com sucesso os principais conceitos de sistemas distribu√≠dos:
- ‚úÖ Replica√ß√£o autom√°tica de escritas.
- ‚úÖ Elei√ß√£o de l√≠der e toler√¢ncia a falhas (Bully Algorithm).
- ‚úÖ Detec√ß√£o de falhas via Heartbeat.
- ‚úÖ Verifica√ß√£o de integridade de dados (MD5).
- ‚úÖ Comunica√ß√£o via sockets em um ambiente de rede.
- ‚úÖ Configura√ß√£o flex√≠vel para se adaptar a diferentes topologias de rede.
